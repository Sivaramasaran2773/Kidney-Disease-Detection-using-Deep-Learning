{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations for training and validation data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load your dataset using torchvision's ImageFolder\n",
    "data_dir = '/kaggle/input/final-data/FinalData'\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Split the dataset into training, testing, and validation sets\n",
    "train_size = 0.7\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "train_dataset, test_val_dataset = train_test_split(dataset, train_size=train_size, shuffle=True, random_state=42)\n",
    "test_dataset, val_dataset = train_test_split(test_val_dataset, train_size=test_size/(test_size + val_size), shuffle=True, random_state=42)\n",
    "\n",
    "# Create separate data loaders for training and validation\n",
    "train_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define the CNN model architecture\n",
    "class CNN(nn.Module):\n",
    "    def _init_(self):\n",
    "        super(CNN, self)._init_()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer\n",
    "        self.fc2 = nn.Linear(512, 4)  # Output classes: 4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        features = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(features)\n",
    "        return x, features\n",
    "\n",
    "# Initialize the CNN model\n",
    "model_cnn = CNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion_cnn = nn.CrossEntropyLoss()\n",
    "weight_decay = 0.001  # L2 regularization (weight decay)\n",
    "optimizer_cnn = optim.Adam(model_cnn.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "model_cnn.to(device)\n",
    "\n",
    "# Initialize variables to store the highest accuracy and corresponding metrics\n",
    "highest_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "best_confusion_matrix = None\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "num_epochs = 100\n",
    "average_f1_score = 0.0  # Initialize average F1 score\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_cnn.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer_cnn.zero_grad()\n",
    "        outputs, _ = model_cnn(inputs)\n",
    "        loss = criterion_cnn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_cnn.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Print training loss\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accuracy = correct / total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {running_loss / len(train_loader)}, Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "    # Validation\n",
    "    model_cnn.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs, _ = model_cnn(inputs)\n",
    "            loss = criterion_cnn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            ground_truths.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate accuracy and F1 score for the current epoch\n",
    "        accuracy = correct / total\n",
    "        f1 = f1_score(ground_truths, predictions, average='weighted')\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        # Print F1 score\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "\n",
    "        # Accumulate total correct predictions and total samples\n",
    "        total_correct += correct\n",
    "        total_samples += total\n",
    "\n",
    "        # Update average F1 score\n",
    "        average_f1_score += f1\n",
    "\n",
    "        # Check if the current accuracy is higher than the highest accuracy\n",
    "        if accuracy > highest_accuracy:\n",
    "            highest_accuracy = accuracy\n",
    "            best_epoch = epoch\n",
    "            best_confusion_matrix = confusion_matrix(ground_truths, predictions)\n",
    "            best_f1_score = f1  # Store the F1 score of the best model\n",
    "\n",
    "            # Save the best model checkpoint\n",
    "            best_model_path = 'best_cnn_model.pth'\n",
    "            torch.save(model_cnn.state_dict(), best_model_path)\n",
    "\n",
    "# Calculate the final average accuracy\n",
    "final_avg_accuracy = total_correct / total_samples\n",
    "\n",
    "# Calculate the average F1 score across all epochs\n",
    "average_f1_score /= num_epochs\n",
    "\n",
    "print(f\"Training Average Accuracy: {100 * final_avg_accuracy:.2f}%\")\n",
    "print(f\"Training Average F1 Score: {average_f1_score:.4f}\")\n",
    "\n",
    "# Print the accuracy and F1 score of the best model\n",
    "print(f\"Best Model Accuracy: {highest_accuracy:.4f}\")\n",
    "print(f\"Best Model F1 Score: {best_f1_score:.4f}\")\n",
    "\n",
    "# Plot training and validation loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load the best model checkpoint for evaluation\n",
    "model_cnn = CNN()\n",
    "model_cnn.load_state_dict(torch.load(best_model_path))\n",
    "model_cnn.to(device)\n",
    "\n",
    "# Initialize variables to store true labels and predicted probabilities for each class\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "\n",
    "# Evaluate the model on the test set to get true labels and predicted probabilities\n",
    "model_cnn.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs, _ = model_cnn(inputs)\n",
    "        probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_labels = np.array(all_labels)\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "# Calculate accuracy, f1-score, and confusion matrix for the test set\n",
    "predictions = np.argmax(all_probabilities, axis=1)\n",
    "accuracy = np.mean(predictions == all_labels)\n",
    "f1 = f1_score(all_labels, predictions, average='weighted')\n",
    "conf_matrix = confusion_matrix(all_labels, predictions)\n",
    "\n",
    "# Calculate precision, recall, f1-score, and support for each class\n",
    "report = classification_report(all_labels, predictions, target_names=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Extract features from the test set for t-SNE visualization\n",
    "model_cnn.eval()\n",
    "all_features = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs, features = model_cnn(inputs)\n",
    "        all_features.extend(features.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_features = np.array(all_features)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Apply t-SNE to the extracted features\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_features = tsne.fit_transform(all_features)\n",
    "\n",
    "# Plot t-SNE results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=tsne_features[:, 0], y=tsne_features[:, 1], hue=all_labels, palette='viridis', s=50)\n",
    "plt.title('t-SNE Visualization of CNN Features')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(title='Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESNET18\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations for training and validation data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load your dataset using torchvision's ImageFolder\n",
    "data_dir = '/kaggle/input/final-data1/FinalData'\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Split the dataset into training, testing, and validation sets\n",
    "train_size = 0.7\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "train_dataset, test_val_dataset = train_test_split(dataset, train_size=train_size, shuffle=True, random_state=42)\n",
    "test_dataset, val_dataset = train_test_split(test_val_dataset, train_size=test_size/(test_size + val_size), shuffle=True, random_state=42)\n",
    "\n",
    "# Create separate data loaders for training and validation\n",
    "train_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize the ResNet model\n",
    "model_resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the fully connected layer (classifier) for the model\n",
    "num_ftrs = model_resnet.fc.in_features\n",
    "dropout_prob = 0.5  # Dropout probability\n",
    "model_resnet.fc = nn.Sequential(\n",
    "    nn.Dropout(p=dropout_prob),\n",
    "    nn.Linear(num_ftrs, 4)  # Output size is 4 for 4 classes\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion_resnet = nn.CrossEntropyLoss()\n",
    "weight_decay = 0.001  # L2 regularization (weight decay)\n",
    "optimizer_resnet = optim.Adam(model_resnet.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "model_resnet.to(device)\n",
    "\n",
    "# Initialize variables to store the highest accuracy and corresponding metrics\n",
    "highest_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "best_confusion_matrix = None\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "num_epochs = 100\n",
    "average_f1_score = 0.0  # Initialize average F1 score\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_resnet.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer_resnet.zero_grad()\n",
    "        outputs = model_resnet(inputs)\n",
    "        loss = criterion_resnet(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_resnet.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Print training loss\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accuracy = correct / total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {running_loss / len(train_loader)}, Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "    # Validation\n",
    "    model_resnet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_resnet(inputs)\n",
    "            loss = criterion_resnet(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            ground_truths.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate accuracy and F1 score for the current epoch\n",
    "        accuracy = correct / total\n",
    "        f1 = f1_score(ground_truths, predictions, average='weighted')\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        # Print F1 score\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "\n",
    "        # Accumulate total correct predictions and total samples\n",
    "        total_correct += correct\n",
    "        total_samples += total\n",
    "\n",
    "        # Update average F1 score\n",
    "        average_f1_score += f1\n",
    "\n",
    "        # Check if the current accuracy is higher than the highest accuracy\n",
    "        if accuracy > highest_accuracy:\n",
    "            highest_accuracy = accuracy\n",
    "            best_epoch = epoch\n",
    "            best_confusion_matrix = confusion_matrix(ground_truths, predictions)\n",
    "            best_f1_score = f1  # Store the F1 score of the best model\n",
    "\n",
    "            # Save the best model checkpoint\n",
    "            best_model_path = 'best_resnet_model.pth'\n",
    "            torch.save(model_resnet.state_dict(), best_model_path)\n",
    "\n",
    "# Calculate the final average accuracy\n",
    "final_avg_accuracy = total_correct / total_samples\n",
    "\n",
    "# Calculate the average F1 score across all epochs\n",
    "average_f1_score /= num_epochs\n",
    "\n",
    "print(f\"Training Average Accuracy: {100 * final_avg_accuracy:.2f}%\")\n",
    "print(f\"Training Average F1 Score: {average_f1_score:.4f}\")\n",
    "\n",
    "# Print the accuracy and F1 score of the best model\n",
    "print(f\"Best Model Accuracy: {highest_accuracy:.4f}\")\n",
    "print(f\"Best Model F1 Score: {best_f1_score:.4f}\")\n",
    "\n",
    "# Plot training and validation loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load the best model checkpoint for evaluation\n",
    "model_resnet = models.resnet18(pretrained=False)\n",
    "model_resnet.fc = nn.Sequential(\n",
    "    nn.Dropout(p=dropout_prob),\n",
    "    nn.Linear(num_ftrs, 4)\n",
    ")\n",
    "model_resnet.load_state_dict(torch.load(best_model_path))\n",
    "model_resnet.to(device)\n",
    "\n",
    "# Initialize variables to store true labels and predicted probabilities for each class\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "\n",
    "# Evaluate the model on the test set to get true labels and predicted probabilities\n",
    "model_resnet.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_resnet(inputs)\n",
    "        probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_labels = np.array(all_labels)\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "# Calculate accuracy, f1-score, and confusion matrix for the test set\n",
    "predictions = np.argmax(all_probabilities, axis=1)\n",
    "accuracy = np.mean(predictions == all_labels)\n",
    "f1 = f1_score(all_labels, predictions, average='weighted')\n",
    "conf_matrix = confusion_matrix(all_labels, predictions)\n",
    "\n",
    "# Calculate precision, recall, f1-score, and support for each class\n",
    "report = classification_report(all_labels, predictions, target_names=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# t-SNE plot\n",
    "\n",
    "# Remove the final fully connected layer for feature extraction\n",
    "feature_extractor = nn.Sequential(*list(model_resnet.children())[:-1])\n",
    "feature_extractor.to(device)\n",
    "\n",
    "# Extract features from the test set\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "model_resnet.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, label in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = feature_extractor(inputs)\n",
    "        outputs = outputs.view(outputs.size(0), -1)  # Flatten the outputs\n",
    "        features.append(outputs.cpu().numpy())\n",
    "        labels.extend(label.numpy())\n",
    "\n",
    "# Convert features and labels to numpy arrays\n",
    "features = np.concatenate(features, axis=0)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Apply t-SNE to the extracted features\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(features)\n",
    "\n",
    "# Plot the t-SNE results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    x=tsne_results[:, 0], y=tsne_results[:, 1],\n",
    "    hue=labels,\n",
    "    palette=sns.color_palette(\"hsv\", 4),\n",
    "    legend=\"full\",\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title('t-SNE Plot')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(title='Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQUEEZENET\n",
    "\n",
    "#SQUEEZENET\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations for training and validation data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load your dataset using torchvision's ImageFolder\n",
    "data_dir = '/kaggle/input/final-data/FinalData'\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Split the dataset into training, testing, and validation sets\n",
    "train_size = 0.7\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "train_dataset, test_val_dataset = train_test_split(dataset, train_size=train_size, shuffle=True, random_state=42)\n",
    "test_dataset, val_dataset = train_test_split(test_val_dataset, train_size=test_size/(test_size + val_size), shuffle=True, random_state=42)\n",
    "\n",
    "# Create separate data loaders for training and validation\n",
    "train_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize the SqueezeNet model\n",
    "model_squeezenet = models.squeezenet1_0(pretrained=True)\n",
    "\n",
    "# Add dropout layer\n",
    "model_squeezenet.classifier[1] = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),  # Increase dropout rate\n",
    "    nn.Conv2d(512, 4, kernel_size=(1, 1), stride=(1, 1))  # Modify the classification head\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion_squeezenet = nn.CrossEntropyLoss()\n",
    "weight_decay = 0.001  # L2 regularization (weight decay)\n",
    "optimizer_squeezenet = optim.Adam(model_squeezenet.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "model_squeezenet.to(device)\n",
    "\n",
    "# Initialize variables to store the highest accuracy and corresponding metrics\n",
    "highest_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "best_confusion_matrix = None\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "num_epochs = 100\n",
    "average_f1_score = 0.0  # Initialize average F1 score\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_squeezenet.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer_squeezenet.zero_grad()\n",
    "        outputs = model_squeezenet(inputs)\n",
    "        loss = criterion_squeezenet(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_squeezenet.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Print training loss\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accuracy = correct / total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {running_loss / len(train_loader)}, Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "    # Validation\n",
    "    model_squeezenet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_squeezenet(inputs)\n",
    "            loss = criterion_squeezenet(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            ground_truths.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate accuracy and F1 score for the current epoch\n",
    "        accuracy = correct / total\n",
    "        f1 = f1_score(ground_truths, predictions, average='weighted')\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        # Print F1 score\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "\n",
    "        # Accumulate total correct predictions and total samples\n",
    "        total_correct += correct\n",
    "        total_samples += total\n",
    "\n",
    "        # Update average F1 score\n",
    "        average_f1_score += f1\n",
    "\n",
    "        # Check if the current accuracy is higher than the highest accuracy\n",
    "        if accuracy > highest_accuracy:\n",
    "            highest_accuracy = accuracy\n",
    "            best_epoch = epoch\n",
    "            best_confusion_matrix = confusion_matrix(ground_truths, predictions)\n",
    "            best_f1_score = f1  # Store the F1 score of the best model\n",
    "\n",
    "            # Save the best model checkpoint\n",
    "            best_model_path = 'best_squeezenet_model.pth'\n",
    "            torch.save(model_squeezenet.state_dict(), best_model_path)\n",
    "\n",
    "# Calculate the final average accuracy\n",
    "final_avg_accuracy = total_correct / total_samples\n",
    "\n",
    "# Calculate the average F1 score across all epochs\n",
    "average_f1_score /= num_epochs\n",
    "\n",
    "print(f\"Training Average Accuracy: {100 * final_avg_accuracy:.2f}%\")\n",
    "print(f\"Training Average F1 Score: {average_f1_score:.4f}\")\n",
    "\n",
    "# Print the accuracy and F1 score of the best model\n",
    "print(f\"Best Model Accuracy: {highest_accuracy:.4f}\")\n",
    "print(f\"Best Model F1 Score: {best_f1_score:.4f}\")\n",
    "\n",
    "# Plot training and validation loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load the best model checkpoint for evaluation\n",
    "model_squeezenet = models.squeezenet1_0(pretrained=False)\n",
    "model_squeezenet.classifier[1] = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Conv2d(512, 4, kernel_size=(1, 1), stride=(1, 1))\n",
    ")\n",
    "model_squeezenet.load_state_dict(torch.load(best_model_path))\n",
    "model_squeezenet.to(device)\n",
    "\n",
    "# Initialize variables to store true labels and predicted probabilities for each class\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "all_features = []\n",
    "\n",
    "# Evaluate the model on the test set to get true labels and predicted probabilities\n",
    "model_squeezenet.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_squeezenet(inputs)\n",
    "        probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "        features = model_squeezenet.features(inputs).cpu().numpy()\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "        all_features.extend(features)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_labels = np.array(all_labels)\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "all_features = np.array(all_features)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reshape features to 2D array (samples, features)\n",
    "all_features_flat = all_features.reshape(all_features.shape[0], -1)\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=50)  # Adjust the number of components as needed\n",
    "all_features_pca = pca.fit_transform(all_features_flat)\n",
    "\n",
    "# Compute t-SNE for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(all_features_pca)\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    x=tsne_results[:, 0], y=tsne_results[:, 1],\n",
    "    hue=all_labels,\n",
    "    palette=sns.color_palette(\"hsv\", len(np.unique(all_labels))),\n",
    "    legend=\"full\",\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('t-SNE Visualization of Test Data')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(title='Class')\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy, f1-score, and confusion matrix for the test set\n",
    "predictions = np.argmax(all_probabilities, axis=1)\n",
    "accuracy = np.mean(predictions == all_labels)\n",
    "f1 = f1_score(all_labels, predictions, average='weighted')\n",
    "conf_matrix = confusion_matrix(all_labels, predictions)\n",
    "\n",
    "# Calculate precision, recall, f1-score, and support for each class\n",
    "report = classification_report(all_labels, predictions, target_names=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
